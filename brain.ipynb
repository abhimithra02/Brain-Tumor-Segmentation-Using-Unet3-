{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "import os\n",
    "import cv2\n",
    "from itertools import chain\n",
    "from tqdm import tqdm_notebook,tnrange\n",
    "from glob import glob\n",
    "from skimage.io import imread, imshow,concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from skimage.color import rgb2gray\n",
    "from tensorflow.keras.models import Model, load_model, save_model\n",
    "from tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataPath = 'A:/New folder/Brain/'\n",
    "\n",
    "dirs = []\n",
    "images = []\n",
    "masks = []\n",
    "for dirname, _, filenames in os.walk(DataPath):\n",
    "    for filename in filenames:\n",
    "        if 'mask'in filename:\n",
    "            dirs.append(dirname.replace(DataPath, ''))\n",
    "            masks.append(filename)\n",
    "            images.append(filename.replace('_mask', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(masks[:10], images[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dirs), len(images), len(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePath_df = pd.DataFrame({'directory':dirs, 'images':images,'masks':masks})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePath_df.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_imShape():\n",
    "    idx = random.randint(0,len(dirs))\n",
    "    \n",
    "    imagePath = os.path.join(DataPath,imagePath_df['directory'].iloc[idx],  imagePath_df['images'].iloc[idx])\n",
    "    maskPath = os.path.join(DataPath,imagePath_df['directory'].iloc[idx],  imagePath_df['masks'].iloc[idx])\n",
    "    \n",
    "    image = cv2.imread(imagePath)\n",
    "    mask = cv2.imread(maskPath)\n",
    "    \n",
    "    print(image.shape)\n",
    "    print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print_imShape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images():\n",
    "    \n",
    "    idx = random.randint(0,len(imagePath_df))\n",
    "    \n",
    "    imagePath = os.path.join(DataPath,imagePath_df['directory'].iloc[idx],  imagePath_df['images'].iloc[idx])\n",
    "    maskPath = os.path.join(DataPath,imagePath_df['directory'].iloc[idx],  imagePath_df['masks'].iloc[idx])\n",
    "    \n",
    "    image = cv2.imread(imagePath)\n",
    "    mask = cv2.imread(maskPath)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,3, figsize=[13,15])\n",
    "    \n",
    "    axs[0].imshow(image)\n",
    "    axs[0].set_title(\"Brain MRI Slice\")\n",
    "    \n",
    "    plt.grid(False)\n",
    "    \n",
    "    axs[1].imshow(mask)\n",
    "    axs[1].set_title('Mask')\n",
    "    \n",
    "    plt.grid(False)\n",
    "    \n",
    "    axs[2].imshow(image)\n",
    "    axs[2].imshow(mask, alpha=0.3)\n",
    "    axs[2].set_title('MRI with mask')\n",
    "    \n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plot_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePath_df['image-path'] = DataPath + imagePath_df['directory'] + '/' + imagePath_df['images']\n",
    "imagePath_df['mask-path'] = DataPath + imagePath_df['directory'] + '/' + imagePath_df['masks'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(imagePath_df,test_size=0.25, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "35*32 == len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 15\n",
    "ImgHeight = 64\n",
    "ImgWidth = 64\n",
    "Channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = dict(rotation_range=0.2,\n",
    "                        width_shift_range=0.05,\n",
    "                        height_shift_range=0.05,\n",
    "                        shear_range=0.05,\n",
    "                        zoom_range=0.05,\n",
    "                        horizontal_flip=True,\n",
    "                        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imagegen = ImageDataGenerator(rescale=1./255., **data_augmentation)\n",
    "maskgen = ImageDataGenerator(rescale=1./255., **data_augmentation)\n",
    "\n",
    "timage_generator=imagegen.flow_from_dataframe(dataframe=train,\n",
    "                                            x_col=\"image-path\",\n",
    "                                            batch_size= BATCH_SIZE,\n",
    "                                            seed=42,\n",
    "                                            class_mode=None,\n",
    "                                            target_size=(ImgHeight,ImgWidth),\n",
    "                                            color_mode='rgb')\n",
    "\n",
    "tmask_generator=maskgen.flow_from_dataframe(dataframe=train,\n",
    "                                            x_col=\"mask-path\",\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            seed=42,\n",
    "                                            class_mode=None,\n",
    "                                            target_size=(ImgHeight,ImgWidth),\n",
    "                                            color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image generator\n",
    "imagegen = ImageDataGenerator(rescale=1./255.)\n",
    "maskgen = ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "vimage_generator = imagegen.flow_from_dataframe(dataframe=test,\n",
    "                                             x_col = \"image-path\",\n",
    "                                             batch_size = BATCH_SIZE,\n",
    "                                             seed=42,\n",
    "                                             class_mode=None,\n",
    "                                             target_size = (ImgHeight,ImgWidth),\n",
    "                                             color_mode='rgb')\n",
    "\n",
    "vmask_generator = maskgen.flow_from_dataframe(dataframe=test,\n",
    "                                             x_col = \"mask-path\",\n",
    "                                             batch_size = BATCH_SIZE,\n",
    "                                             seed=42,\n",
    "                                             class_mode=None,\n",
    "                                             target_size = (ImgHeight,ImgWidth),\n",
    "                                             color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iterator(image_gen,mask_gen):\n",
    "    for img, mask in zip(image_gen,mask_gen):\n",
    "        yield img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = data_iterator(timage_generator, tmask_generator)\n",
    "val_gen = data_iterator(vimage_generator,vmask_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, kernels, kernel_size=(3, 3), strides=(1, 1), padding='same', is_bn=True, is_relu=True, n=2):\n",
    "    for i in range(1, n + 1):\n",
    "        x = k.layers.Conv2D(filters=kernels, kernel_size=kernel_size, padding=padding, strides=strides,\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                            kernel_initializer=k.initializers.he_normal(seed=5))(x)\n",
    "        if is_bn:\n",
    "            x = k.layers.BatchNormalization()(x)\n",
    "        if is_relu:\n",
    "            x = k.activations.relu(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def dotProduct(seg, cls):\n",
    "    B, H, W, N = k.backend.int_shape(seg)\n",
    "    seg = tf.reshape(seg, [-1, H*W, N])\n",
    "    final = tf.einsum(\"ijk,ik->ijk\", seg, cls)\n",
    "    final = tf.reshape(final, [-1, H, W, N])\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet_3Plus(INPUT_SHAPE, OUTPUT_CHANNELS):\n",
    "    filters = [64, 128, 256, 512, 1024]\n",
    "\n",
    "    input_layer = k.layers.Input(shape=INPUT_SHAPE, name=\"input_layer\")  # 320*320*3\n",
    "\n",
    "    \"\"\" Encoder\"\"\"\n",
    "    # block 1\n",
    "    e1 = conv_block(input_layer, filters[0])        # 320*320*64\n",
    "\n",
    "    # block 2\n",
    "    e2 = k.layers.MaxPool2D(pool_size=(2, 2))(e1)  # 160*160*64\n",
    "    e2 = conv_block(e2, filters[1])                 # 160*160*128\n",
    "\n",
    "    # block 3\n",
    "    e3 = k.layers.MaxPool2D(pool_size=(2, 2))(e2)  # 80*80*128\n",
    "    e3 = conv_block(e3, filters[2])                 # 80*80*256\n",
    "\n",
    "    # block 4\n",
    "    e4 = k.layers.MaxPool2D(pool_size=(2, 2))(e3)  # 40*40*256\n",
    "    e4 = conv_block(e4, filters[3])                 # 40*40*512\n",
    "\n",
    "    # block 5\n",
    "    # bottleneck layer\n",
    "    e5 = k.layers.MaxPool2D(pool_size=(2, 2))(e4)  # 20*20*512\n",
    "    e5 = conv_block(e5, filters[4])                 # 20*20*1024\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    cat_channels = filters[0]\n",
    "    cat_blocks = len(filters)\n",
    "    upsample_channels = cat_blocks * cat_channels\n",
    "\n",
    "    \"\"\" d4 \"\"\"\n",
    "    e1_d4 = k.layers.MaxPool2D(pool_size=(8, 8))(e1)                        # 320*320*64  --> 40*40*64\n",
    "    e1_d4 = conv_block(e1_d4, cat_channels, n=1)                       # 320*320*64  --> 40*40*64\n",
    "\n",
    "    e2_d4 = k.layers.MaxPool2D(pool_size=(4, 4))(e2)                        # 160*160*128 --> 40*40*128\n",
    "    e2_d4 = conv_block(e2_d4, cat_channels, n=1)                        # 160*160*128 --> 40*40*64\n",
    "\n",
    "    e3_d4 = k.layers.MaxPool2D(pool_size=(2, 2))(e3)                        # 80*80*256  --> 40*40*256\n",
    "    e3_d4 = conv_block(e3_d4, cat_channels, n=1)                        # 80*80*256  --> 40*40*64\n",
    "\n",
    "    e4_d4 = conv_block(e4, cat_channels, n=1)                          # 40*40*512  --> 40*40*64\n",
    "\n",
    "    e5_d4 = k.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(e5)  # 80*80*256  --> 40*40*256\n",
    "    e5_d4 = conv_block(e5_d4, cat_channels, n=1)                        # 20*20*1024  --> 20*20*64\n",
    "\n",
    "    d4 = k.layers.concatenate([e1_d4, e2_d4, e3_d4, e4_d4, e5_d4])\n",
    "    d4 = conv_block(d4, upsample_channels, n=1)                         # 40*40*320  --> 40*40*320\n",
    "\n",
    "    \"\"\" d3 \"\"\"\n",
    "    e1_d3 = k.layers.MaxPool2D(pool_size=(4, 4))(e1)    # 320*320*64 --> 80*80*64\n",
    "    e1_d3 = conv_block(e1_d3, cat_channels, n=1)        # 80*80*64 --> 80*80*64\n",
    "\n",
    "    e2_d3 = k.layers.MaxPool2D(pool_size=(2, 2))(e2)    # 160*160*256 --> 80*80*256\n",
    "    e2_d3 = conv_block(e2_d3, cat_channels, n=1)        # 80*80*256 --> 80*80*64\n",
    "\n",
    "    e3_d3 = conv_block(e3, cat_channels, n=1)           # 80*80*512 --> 80*80*64\n",
    "\n",
    "    d4_d3 = k.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(d4)      # 40*40*320 --> 80*80*320\n",
    "    d4_d3 = conv_block(d4_d3, cat_channels, n=1)        # 80*80*320 --> 80*80*64\n",
    "\n",
    "    e5_d3 = k.layers.UpSampling2D(size=(4, 4), interpolation='bilinear')(e5)      # 20*20*320 --> 80*80*320\n",
    "    e5_d3 = conv_block(e5_d3, cat_channels, n=1)        # 80*80*320 --> 80*80*64\n",
    "\n",
    "    d3 = k.layers.concatenate([e1_d3, e2_d3, e3_d3, d4_d3, e5_d3])\n",
    "    d3 = conv_block(d3, upsample_channels, n=1)         # 80*80*320 --> 80*80*320\n",
    "\n",
    "    \"\"\" d2 \"\"\"\n",
    "    e1_d2 = k.layers.MaxPool2D(pool_size=(2, 2))(e1)    # 320*320*64 --> 160*160*64\n",
    "    e1_d2 = conv_block(e1_d2, cat_channels, n=1)        # 160*160*64 --> 160*160*64\n",
    "\n",
    "    e2_d2 = conv_block(e2, cat_channels, n=1)           # 160*160*256 --> 160*160*64\n",
    "\n",
    "    d3_d2 = k.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(d3)      # 80*80*320 --> 160*160*320\n",
    "    d3_d2 = conv_block(d3_d2, cat_channels, n=1)        # 160*160*320 --> 160*160*64\n",
    "\n",
    "    d4_d2 = k.layers.UpSampling2D(size=(4, 4), interpolation='bilinear')(d4)      # 40*40*320 --> 160*160*320\n",
    "    d4_d2 = conv_block(d4_d2, cat_channels, n=1)        # 160*160*320 --> 160*160*64\n",
    "\n",
    "    e5_d2 = k.layers.UpSampling2D(size=(8, 8), interpolation='bilinear')(e5)      # 20*20*320 --> 160*160*320\n",
    "    e5_d2 = conv_block(e5_d2, cat_channels, n=1)        # 160*160*320 --> 160*160*64\n",
    "\n",
    "    d2 = k.layers.concatenate([e1_d2, e2_d2, d3_d2, d4_d2, e5_d2])\n",
    "    d2 = conv_block(d2, upsample_channels, n=1)         # 160*160*320 --> 160*160*320\n",
    "\n",
    "    \"\"\" d1 \"\"\"\n",
    "    e1_d1 = conv_block(e1, cat_channels, n=1)      # 320*320*64 --> 320*320*64\n",
    "\n",
    "    d2_d1 = k.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(d2)      # 160*160*320 --> 320*320*320\n",
    "    d2_d1 = conv_block(d2_d1, cat_channels, n=1)        # 160*160*320 --> 160*160*64\n",
    "\n",
    "    d3_d1 = k.layers.UpSampling2D(size=(4, 4), interpolation='bilinear')(d3)      # 80*80*320 --> 320*320*320\n",
    "    d3_d1 = conv_block(d3_d1, cat_channels, n=1)        # 320*320*320 --> 320*320*64\n",
    "\n",
    "    d4_d1 = k.layers.UpSampling2D(size=(8, 8), interpolation='bilinear')(d4)      # 40*40*320 --> 320*320*320\n",
    "    d4_d1 = conv_block(d4_d1, cat_channels, n=1)        # 320*320*320 --> 320*320*64\n",
    "\n",
    "    e5_d1 = k.layers.UpSampling2D(size=(16, 16), interpolation='bilinear')(e5)    # 20*20*320 --> 320*320*320\n",
    "    e5_d1 = conv_block(e5_d1, cat_channels, n=1)        # 320*320*320 --> 320*320*64\n",
    "\n",
    "    d1 = k.layers.concatenate([e1_d1, d2_d1, d3_d1, d4_d1, e5_d1, ])\n",
    "    d1 = conv_block(d1, upsample_channels, n=1)         # 320*320*320 --> 320*320*320\n",
    "\n",
    "    # last layer does not have batchnorm and relu\n",
    "    d = conv_block(d1, OUTPUT_CHANNELS, n=1, is_bn=False, is_relu=False)\n",
    "\n",
    "    if OUTPUT_CHANNELS == 1:\n",
    "        output = k.activations.sigmoid(d)\n",
    "    else:\n",
    "        output = k.activations.softmax(d)\n",
    "\n",
    "    return tf.keras.Model(inputs=input_layer, outputs=output, name='UNet_3Plus')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \"\"\"## Model Compilation\"\"\"\n",
    "    INPUT_SHAPE = [64, 64, 3]\n",
    "    OUTPUT_CHANNELS = 1\n",
    "\n",
    "    unet_3P = UNet_3Plus(INPUT_SHAPE, OUTPUT_CHANNELS)\n",
    "    unet_3P.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(ytrue, ypred):\n",
    "    smoothing_factor=0.1\n",
    "    #y_true_f=K.flatten(y_true)\n",
    "    #y_pred_f=K.flatten(y_pred)\n",
    "    intersection = K.sum(ytrue*ypred)\n",
    "    combined_area = K.sum(ytrue+ypred)\n",
    "    union_area = combined_area - intersection\n",
    "    iou = (intersection+smoothing_factor)/(union_area+smoothing_factor)\n",
    "    return iou\n",
    "\n",
    "def jac_distance(y_true, y_pred):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    \n",
    "    return -iou(y_true, y_pred)\n",
    "\n",
    "# Dice Sorenson\n",
    "def dice_coef(ytrue, ypred):\n",
    "    smoothing_factor=0.1\n",
    "    ytrue_f = K.flatten(ytrue)\n",
    "    ypred_f = K.flatten(ypred)\n",
    "    intersection = K.sum(ytrue*ypred)\n",
    "    ytrue_area = K.sum(ytrue)\n",
    "    ypred_area = K.sum(ypred)\n",
    "    combined_area = ytrue_area + ypred_area\n",
    "    dice = 2*((intersection+smoothing_factor)/(combined_area+smoothing_factor))\n",
    "    return dice\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_3P.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=[\"accuracy\", iou, dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=1e-5, verbose=1),\n",
    "    ModelCheckpoint('model-brain-mri.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]\n",
    "\n",
    "STEP_SIZE_TRAIN = timage_generator.n/BATCH_SIZE\n",
    "STEP_SIZE_VALID = vimage_generator.n/BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = unet_3P.fit(train_gen,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=val_gen,\n",
    "                   validation_steps=STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,8])\n",
    "plt.title(\"Loss Function\")\n",
    "plt.plot(hist.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "\n",
    "plt.plot( np.argmin(hist.history[\"val_loss\"]), np.min(hist.history[\"val_loss\"]), marker=\"o\", color=\"b\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend()\n",
    "# plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Loss Function\")\n",
    "\n",
    "plt.title('Training and validation Jacard')\n",
    "plt.plot(hist.history[\"iou\"], 'y', label='Training Jacard')\n",
    "plt.plot(hist.history['val_iou'], 'r', label='Validation Jacard')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Jacard')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,5])\n",
    "plt.title(\"dice_coef\")\n",
    "plt.plot(hist.history[\"dice_coef\"], label = \"dice_coef\")\n",
    "plt.plot(hist.history[\"val_dice_coef\"], label = \"val_dice_coef\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Dice Coefficient\")\n",
    "plt.legend()\n",
    "# plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = unet_3P.evaluate(val_gen, steps=STEP_SIZE_VALID, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    idx = random.randint(0,len(imagePath_df))\n",
    "    \n",
    "    imagePath = os.path.join(DataPath, imagePath_df['directory'].iloc[idx], imagePath_df['images'].iloc[idx])\n",
    "    maskPath = os.path.join(DataPath, imagePath_df['directory'].iloc[idx], imagePath_df['masks'].iloc[idx])\n",
    "    \n",
    "    image = cv2.imread(imagePath)\n",
    "    mask = cv2.imread(maskPath)\n",
    "    \n",
    "    img = cv2.resize(image,(ImgHeight,ImgWidth))\n",
    "    img = img/255\n",
    "    img = img[np.newaxis,:,:,:]\n",
    "    pred=unet_3P.predict(img)\n",
    "    \n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(np.squeeze(img))\n",
    "    plt.title('Original Image')\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(mask)\n",
    "    plt.title('Original Mask')\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(np.squeeze(pred))\n",
    "    plt.title('Prediction')\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(np.squeeze(pred) > 0.5)\n",
    "    plt.title('BinaryPrediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a1a1bd5ae6b59f4e0807aad899fba668fa191c1e4e2fb636dcdc16d38bbcca5"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
